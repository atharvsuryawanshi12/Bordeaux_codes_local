{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate-coded version of the dual pathway model\n",
    "\n",
    "This script creates a sensorimotor landscapes by superimposing several Gaussians, and tests the dual pathway model on it.\n",
    "\n",
    "The model consists of two parallel pathways:-\n",
    "- one: performs RL and is reset every day (exploration)\n",
    "- two: maintains a record of the BG exploration (exploitation)\n",
    "\n",
    "The reset at night depends on the potentiation observed during the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auxilliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to generate the sensorimotor landscape on which the model is tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient(n=256, contour_type='Artificial'):\n",
    "        \"\"\" Redirects to the function that generates the desired contour type \"\"\"\n",
    "        \n",
    "        if contour_type == 'Syrinx': return generate_gradient_syrinx(n)\n",
    "        elif contour_type == 'Artificial': return generate_gradient_artificial(n)\n",
    "        else: print(\"Contour type not specified (Syrinx/Artificial).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient_artificial(n=256, n_distractors=20):\n",
    "        \"\"\" Creates the overall reward contour by combining several gaussians. \"\"\"\n",
    "        \n",
    "        # No. of local optima\n",
    "        n_hills = n_distractors\n",
    "\n",
    "        # Target i.e. global optima\n",
    "        # target_theta = np.random.uniform(0,2*np.pi)\n",
    "        # target_r = np.random.uniform(0,1)\n",
    "        # targetpos = target_r*np.cos(target_theta), target_r*np.sin(target_theta)\n",
    "        targetpos = np.random.uniform(-1, 1, 2)\n",
    "        hills = [hill(0.3, targetpos)] # chosen sigma=0.3\n",
    "\n",
    "        # n_hills distractors (0.4 < sigma < 0.7) i.e. local optima\n",
    "        for i in range(n_hills):\n",
    "            hills.append(hill(np.random.uniform(0.4, 0.7)))\n",
    "\n",
    "        # Build gradient landscape\n",
    "        Z = np.zeros((n,n))\n",
    "        for (center, sigma) in hills:\n",
    "            Z = np.maximum(Z, gaussian(n, center, sigma))\n",
    "            # Z = Z + gaussian(n, center, sigma)    # For a smoother reward profile    \n",
    "        \n",
    "        return  Z / Z.max(), targetpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill(sigma=0.1, center=None):\n",
    "        \"\"\" Randomly assigns the mean and std deviation for the hills in the reward contour. \"\"\"\n",
    "\n",
    "        if center is None:\n",
    "            # r = np.sqrt(np.random.uniform(0.0, 1.0))\n",
    "            # a = np.random.uniform(0, 2*np.pi)\n",
    "            # center = r*np.cos(a), r*np.sin(a)\n",
    "            center = np.random.uniform(-1, 1, 2)\n",
    "\n",
    "        return center, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(n=128, center=(0,0), sigma=0.1):\n",
    "        \"\"\" Creates a 2D gaussian distribution, given the center coordinates and std deviation. \"\"\"\n",
    "\n",
    "        X, Y = np.meshgrid(np.linspace(-1, +1, n), np.linspace(-1, +1, n))\n",
    "        x0, y0 = center\n",
    "        D = np.sqrt((X-x0)**2/(2*sigma**2) + (Y-y0)**2/(2*sigma**2))\n",
    "\n",
    "        return 1/(2*np.pi*sigma**2)*np.exp(-D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gradient(Z, p):\n",
    "        \"\"\" Returns height of reward profile at given position. \"\"\"\n",
    "\n",
    "        x = min(max(p[0], -1), 1)\n",
    "        y = min(max(p[1], -1), 1)\n",
    "        col = int(((x + 1)/2) * (Z.shape[1]-1))\n",
    "        row = int(((y + 1)/2) * (Z.shape[0]-1))\n",
    "\n",
    "        return Z[row, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions.\n",
    "\n",
    "Steep sigmoidals help in making neurons function as essentially binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, m=5, a=0.5):\n",
    "    \"\"\" Returns an output between 0 and 1. \"\"\"\n",
    "    return 1 / (1 + np.exp(-1*(x-a)*m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_neg(x, m=5, a=0.5):\n",
    "    \"\"\" Returns an output between -1 and 1. \"\"\"\n",
    "    return (2 / (1 + np.exp(-1*(x-a)*m))) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate of motor pathway.\n",
    "\n",
    "Vestigial code.\n",
    "Used previously to test changing learning rates in the cortical pathway over development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pPos(t):\n",
    "    \"\"\" Testing changing learning rate of HL weights \"\"\" \n",
    "\n",
    "    return 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main function to simulate learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_simulate(arg_rseed, arg_annealing=True, arg_plot=True):\n",
    "    \"\"\" Main function to build and simulate the model \"\"\"\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Parameters\n",
    "    # ---------- #\n",
    "\n",
    "    # Random seed parameters\n",
    "    rSeed = int(arg_rseed)\n",
    "    np.random.seed(rSeed)\n",
    "\n",
    "    # Experiment parameterss\n",
    "    annealing = arg_annealing\n",
    "    plotting = arg_plot\n",
    "\n",
    "    # Output parameters\n",
    "    resname = str(rSeed)\n",
    "    if annealing == True: resname = resname + '_annealing'\n",
    "\n",
    "    print(resname)\n",
    "\n",
    "    # Layer parameters\n",
    "    HVC_total_size = 100              \n",
    "    BG_size = 50\n",
    "    # RA_inh_size = 2\n",
    "    RA_size = 100\n",
    "    MC_size = 2\n",
    "    n_BG_clusters = 10\n",
    "    n_RA_clusters = MC_size\n",
    "\n",
    "    # Weight parameters\n",
    "    soft_bound = False                          \n",
    "    Wmin_f, Wmax_f = 0, 1\n",
    "    Wmin_RL, Wmax_RL = -1, 1\n",
    "    Wmin_HL, Wmax_HL = -1, 1\n",
    "    Wepsilon = 0.1\n",
    "\n",
    "    # Input encoding\n",
    "    n_syllables = 1                                            # setting the no. of syllables to simulate; keep in mind the maximum syllables possible without overlap in encoding\n",
    "    n_bits = HVC_total_size//10                                # no. of bits active in each syllable encoding; corresponds with 10% of HVC being active for one timepoint in song\n",
    "    HVC_size = n_bits * n_syllables                            # effective size of HVC\n",
    "\n",
    "\n",
    "    # Simulation parameters\n",
    "    n_daily_motifs = 1000\n",
    "    n_days = 60\n",
    "    n_trial_per_syll = n_daily_motifs * n_days \n",
    "    n_intact_trials = n_trial_per_syll * n_syllables            # Both pathways are active in intact trials\n",
    "    n_lesioned_days = 1                                         # to check Hebbian learning result without BG input\n",
    "    n_total_trials = n_intact_trials + n_lesioned_days * n_daily_motifs * n_syllables\n",
    "\n",
    "    # Layer interactions\n",
    "    RA_inter_neurons = False                                             # To include inhibitory inter-neurons in RA\n",
    "    Hebbian_learning = HEBBIAN_LEARNING = True\n",
    "    BG_influence = True\n",
    "    Hebbian_rule = 1                                                     # Type of learning rule: 1 -> Hebbian, 2 -> iBCM, 3 -> Oja\n",
    "    eta = ETA = 0.1\n",
    "\n",
    "    # Neuron parameters\n",
    "    BG_noise_mean = 0.00                                                 # Activation function parameters\n",
    "    BG_noise_std = 0.05\n",
    "    RA_noise_mean = 0.00\n",
    "    RA_noise_std = 0.01\n",
    "\n",
    "    BG_sig_slope = 10                                                    # BG sigmoidal should be as less steep as possible\n",
    "    BG_sig_mid = 0\n",
    "    RA_sig_slope = 30                                                    # RA sigmoidal should be as steep as possible\n",
    "    RA_sig_mid = 0\n",
    "\n",
    "    # Reward computation parameters: performance is computed with a window over recent trials\n",
    "    reward_window = 25                      \n",
    "    reward_sigma = 0.2\n",
    "\n",
    "\n",
    "    # Model build\n",
    "    # ----------- #\n",
    "    HVC = np.zeros(HVC_size)\n",
    "    RA = np.zeros(RA_size)\n",
    "    MC = np.zeros(MC_size)\n",
    "    BG = np.zeros(BG_size)\n",
    "\n",
    "    # Randomised initialisation\n",
    "    W_HVC_RA = np.zeros((HVC_size, RA_size), float) #+ .5 # Wepsilon                                        # HVC-RA connections should start from non-existent;\n",
    "    W_HVC_BG = np.random.uniform(Wmin_RL + Wepsilon, Wmax_RL - Wepsilon, (HVC_size, BG_size))               # HVC-BG randomly initialised\n",
    "    W_BG_RA = np.random.uniform(Wmin_f + Wepsilon, Wmax_f - Wepsilon, (BG_size, RA_size))                   # BG-RA generated randomly and remains fixed over the simulation\n",
    "    W_RA_MC = np.random.uniform(Wmin_f + Wepsilon, Wmax_f - Wepsilon, (RA_size, MC_size))                   # RA-MC generated randomly and remains fixed over the simulation\n",
    "\n",
    "    # Segregated pathways between RA and MC (topological connections)\n",
    "    RA_cluster_size = RA_size//n_RA_clusters\n",
    "    BG_cluster_size = BG_size//n_BG_clusters\n",
    "    RA_channel_size = RA_size//n_BG_clusters\n",
    "\n",
    "    for i in range(n_RA_clusters):\n",
    "        segPath = np.diag(np.ones(n_RA_clusters, int))[i]\n",
    "        W_RA_MC[i*RA_cluster_size : (i+1)*RA_cluster_size] *= segPath\n",
    "\n",
    "    # Segregated pathways between BG and RA\n",
    "    for i in range(n_BG_clusters):\n",
    "        segPath = np.diag(np.ones(n_BG_clusters, int))[i]\n",
    "        W_BG_RA[i*BG_cluster_size : (i+1)*BG_cluster_size] *= [j for j in segPath for r in range(RA_channel_size)]\n",
    "\n",
    "    # Sparsity increases as number of connections/neurons increases\n",
    "    # idx1 = np.random.choice(np.arange(W_HVC_BG.size), size=int(np.sqrt(W_HVC_BG.size)), replace=False)\n",
    "    # W_HVC_BG[idx1//BG_size, idx1%BG_size] = 0\n",
    "    idx = np.random.choice(np.arange(W_BG_RA.size), size=int(np.sqrt(W_BG_RA.size)), replace=False)\n",
    "    W_BG_RA[idx//RA_size, idx%RA_size] = 0\n",
    "    idx = np.random.choice(np.arange(W_RA_MC.size), size=int(np.sqrt(W_RA_MC.size)), replace=False)\n",
    "    W_RA_MC[idx//MC_size, idx%MC_size] = 0\n",
    "\n",
    "\n",
    "    ## Syllable encoding (input)\n",
    "    syllable_encoding = {}\n",
    "    syllables = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]              # General repertoire\n",
    "    syllables = syllables[:n_syllables]                                         # ensure n_samples >= len(syllables)\n",
    "\n",
    "    # Eg. 3 bit encoding for 2 syllables in a 10 neuron HVC:- \"A\" 1110000000, \"B\" 0001110000\n",
    "    for i in range(n_syllables):\n",
    "        inputs = np.zeros(HVC_size)\n",
    "        inputs[i * n_bits:(i + 1) * n_bits] = 1\n",
    "        syllable_encoding[syllables[i]] = inputs\n",
    "\n",
    "    # Different reward landscape for each syllable\n",
    "    syllable_targets = {}\n",
    "    syllable_contours = {}\n",
    "    for i in range(n_syllables):\n",
    "        contour, target = generate_gradient()\n",
    "        syllable_contours[syllables[i]] = contour\n",
    "        syllable_targets[syllables[i]] = target\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- #\n",
    "\n",
    "    # Tracking\n",
    "    R = np.zeros((n_syllables, n_total_trials//n_syllables))                                # keeps track of reward\n",
    "    W_RL = np.zeros((n_total_trials, n_bits*BG_size))                                       # tracks HVC-BG weights\n",
    "    W_HL = np.zeros((n_total_trials, W_HVC_RA.size))                                        # tracks HVC-RA weights\n",
    "\n",
    "    S_MO = np.zeros((n_total_trials, MC_size))                                              # tracks the 2D output\n",
    "    S_T = np.zeros((n_total_trials, MC_size))                                               # tracks target\n",
    "    \n",
    "    S_RA = np.zeros((n_total_trials, RA_size))                                              # tracks RA activity\n",
    "    S_BG = np.zeros((n_total_trials, BG_size))                                              # tracks BG activity\n",
    "\n",
    "    S_dW1_sum = np.zeros((n_syllables, n_total_trials//n_syllables))                        # tracks sum of change in BG weights over a given day (proxy for measure of potentiation over the day)\n",
    "    S_p = np.zeros((n_total_trials))                                                        # function of S_dW1_sum\n",
    "\n",
    "    S_RA_HVC = np.zeros((n_total_trials, RA_size))                                          # tracks input to each RA neuron from the HVC\n",
    "    S_RA_BG = np.zeros((n_total_trials, RA_size))                                           # tracks input to each RA neuron from the BG\n",
    "\n",
    "    sum_W_HVC_BG = np.sum((W_HVC_BG!=0), axis=0)                                            # Used for normalisation\n",
    "    sum_W_BG_RA = np.sum(np.abs(W_BG_RA), axis=0)                                           # Used for normalisation\n",
    "    sum_W_RA_MC = np.sum(W_RA_MC, axis=0)                                                   # Used for normalisation\n",
    "\n",
    "\n",
    "    RA_HVC = np.zeros((RA_size))                                                            # current input to each RA neuron from the HVC\n",
    "    RA_BG = np.zeros((RA_size))                                                             # current input to each RA neuron from the BG\n",
    "    dW_RL = np.zeros(W_HVC_BG.shape)                                                        # current change to RL weights\n",
    "    dW_HL = np.zeros(W_HVC_RA.shape)                                                        # current change to HL weights\n",
    "    night_noise = np.zeros(W_HVC_BG.shape)                                                  # nightly noise induced within RL weights\n",
    "    dW_night = np.zeros(W_HVC_BG.shape)                                                     # nightly displacement of RL weights\n",
    "\n",
    "\n",
    "    #### Main simulation of learning\n",
    "    nt = 0\n",
    "    inh_lesion = 1\n",
    "    \n",
    "    # Simulate over a some training days + one day with BG lesioned off\n",
    "    for n_day in np.arange(n_days + n_lesioned_days):\n",
    "        day_dW1_sum = np.zeros((n_syllables))                                               # Tracks BG weights potentiation over the day\n",
    "        day_R_sum = np.zeros((n_syllables))                                                 # Tracks reward obtained over the day\n",
    "        \n",
    "        # Turn BG off if training is over\n",
    "        if BG_influence==1 and n_day >= n_days:\n",
    "            BG_influence = 0\n",
    "            \n",
    "        # Simulate a given number of motifs each day\n",
    "        for n_motif in np.arange(n_daily_motifs):\n",
    "             \n",
    "            # (1 motif = 1 iteration over all syllables)\n",
    "            for n_syll in np.arange(n_syllables):\n",
    "                # Tracking iteration number wrt motifs and syllables\n",
    "                n_iter = int(n_day * n_daily_motifs + n_motif)\n",
    "                nt = n_iter * n_syllables + n_syll\n",
    "\n",
    "                # Syllable input and target encoding\n",
    "                syll = syllables[n_syll]                                                    \n",
    "                target = syllable_targets[syll]\n",
    "                contour = syllable_contours[syll]\n",
    "                HVC[...] = syllable_encoding[syll]\n",
    "\n",
    "                \n",
    "                # Compute BG\n",
    "                BG[...] = np.dot(HVC, W_HVC_BG) / sum_W_HVC_BG + np.random.normal(BG_noise_mean, BG_noise_std, BG_size)\n",
    "                BG[...] = sigmoid_neg(BG, BG_sig_slope, BG_sig_mid)\n",
    "            \n",
    "\n",
    "                # Compute RA activity\n",
    "                RA_HVC[...] = np.dot(HVC, W_HVC_RA) / n_bits * Hebbian_learning\n",
    "                RA_BG[...] = np.dot(BG, W_BG_RA) / sum_W_BG_RA * BG_influence * 2\n",
    "        \n",
    "                RA =  ((RA_HVC) * Hebbian_learning) + (RA_BG * BG_influence)\n",
    "                \n",
    "                RA[...] = sigmoid_neg(RA, RA_sig_slope, RA_sig_mid)\n",
    "\n",
    "                RA_noise = np.random.normal(RA_noise_mean, RA_noise_std, RA_size)                                       # Currently this has been set to extremely low - so for now, it is ineffective\n",
    "                RA = RA + RA_noise\n",
    "\n",
    "                # Compute MC activity\n",
    "                MC[...] = (np.dot(RA, W_RA_MC)/sum_W_RA_MC)\n",
    "\n",
    "                # Print 2D output at first trial (initial point)\n",
    "                if nt == 0: print(MC)                                       \n",
    "                \n",
    "\n",
    "                # Compute error and reward\n",
    "                reward = read_gradient(contour, MC)\n",
    "\n",
    "\n",
    "                # Computing mean recent reward\n",
    "                R_ = 0\n",
    "                if n_iter > reward_window: R_ = R[n_syll, n_iter - reward_window:n_iter].mean()\n",
    "                elif n_iter > 0: R_ = R[n_syll, :n_iter].mean()\n",
    "\n",
    "                # Weight update\n",
    "                dW_RL[...] = eta * (reward - R_) * HVC.reshape(HVC_size, 1) * (BG) * BG_influence\n",
    "\n",
    "\n",
    "                # Tau for lBCM (Law and Cooper, 1994)\n",
    "                theta_M = 1\n",
    "                \n",
    "                \n",
    "                if Hebbian_rule == 1:\n",
    "                    dW_HL[...] = pPos(n_iter) * HVC.reshape(HVC_size, 1) * (RA) * Hebbian_learning\n",
    "\n",
    "                elif Hebbian_rule == 2:\n",
    "                    if nt > 500:    theta_M = np.power(np.mean(S_RA[nt-500:nt], axis=0)-.5, 2)\n",
    "                    elif nt > 0:    theta_M = np.power(np.mean(S_RA[:nt], axis=0)-.5, 2)\n",
    "                    dW_HL[...] = pPos(n_iter) * HVC.reshape(HVC_size, 1) * (RA) / (theta_M+Wepsilon) * (RA-theta_M) * Hebbian_learning                                                                  # iBCM\n",
    "\n",
    "                elif Hebbian_rule == 3:\n",
    "                    dW_HL[...] = pPos(n_iter) * ((HVC.reshape(HVC_size, 1) * RA) - (HVC.reshape(HVC_size, 1) * RA * RA * W_HVC_RA)) * Hebbian_learning                                                 # Oja learning rule                                                                  RA_size) * Hebbian_learning\n",
    "\n",
    "                # Track potentiation received\n",
    "                day_dW1_sum[n_syll] += np.mean((np.abs(dW_RL))[n_bits*(n_syll):n_bits*(n_syll+1)])\n",
    "\n",
    "                # Bounding weights (asymptotically/abruptly)\n",
    "                if soft_bound == 1:\n",
    "                    # W_HVC_BG += dW_RL * (Wmax_RL - W_HVC_BG) * (W_HVC_BG - Wmin_RL)\n",
    "                    W_HVC_RA[...] += dW_HL * (Wmax_HL - W_HVC_RA) * (W_HVC_RA - Wmin_HL)\n",
    "                else:\n",
    "                    W_HVC_RA[...] = np.minimum(Wmax_HL, np.maximum(Wmin_HL, W_HVC_RA + dW_HL))\n",
    "                    W_HVC_BG[...] = np.minimum(Wmax_RL, np.maximum(Wmin_RL, W_HVC_BG + dW_RL))\n",
    "                \n",
    "\n",
    "                # For tracking purposes\n",
    "                R[n_syll, n_iter] = reward\n",
    "                W_RL[nt] = W_HVC_BG[:n_bits,:].ravel()                                \n",
    "                W_HL[nt] = W_HVC_RA.ravel()\n",
    "                S_MO[nt] = MC.ravel()\n",
    "                S_T[nt] = target\n",
    "                S_RA[nt] = RA\n",
    "                S_RA_HVC[nt] = RA_HVC\n",
    "                S_RA_BG[nt] = RA_BG\n",
    "                S_BG[nt] = BG\n",
    "                S_dW1_sum[n_syll, n_iter] = day_dW1_sum[n_syll]\n",
    "        \n",
    "\n",
    "        ## Simulating nightly jumps within BG pathway\n",
    "        if annealing == True:\n",
    "            # Night shuffling of HVC-BG synapses\n",
    "            p = day_dW1_sum[n_syll]/10                                                                      # p is proportional to daily sum of change in BG weights\n",
    "            p = sigmoid(p, 10)\n",
    "\n",
    "            # When potentiation is high during a day, we simulate a low disturbance in the night \n",
    "            potentiation_factor = np.zeros((HVC_size))\n",
    "            potentiation_factor[:(n_bits*n_syllables)] = 1-p                                                # potentiation_factor is inversely proportional to p\n",
    "\n",
    "            S_p[nt] = 1-p \n",
    "\n",
    "            night_noise[...] = np.random.uniform(-1, 1, (BG_size)) * BG_noise_std * 1000\n",
    "            dW_night[...] = eta * (potentiation_factor.reshape(HVC_size,1)) * (night_noise) * BG_influence\n",
    "\n",
    "            W_HVC_BG[...] = (W_HVC_BG + dW_night + 1)%2-1                                                   # modulus so that the high noise doesn't make the weights to saturate\n",
    "\n",
    "\n",
    "    if plotting == True:\n",
    "        # Save results for plotting\n",
    "        np.savetxt('Results/'+resname+'_endRA.txt', S_RA[-1]>.5)\n",
    "\n",
    "        test_k = 10                                                                                         # Chooses which particular RA neuron to observe\n",
    "\n",
    "        # # Plot inputs to RA_exc\n",
    "        # plt.close()\n",
    "\n",
    "        # plt.plot(S_RA_BG[:,test_k], label='BG',lw=0.5, marker=',', alpha=.5)\n",
    "        # # plt.plot(S_RA_inh[:,test_k], label='inh',lw=0.5, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA_BG[:,test_k]+S_RA_HVC[:,test_k], label='BG+HVC',lw=0.5, marker=',', alpha=.5)\n",
    "        # # plt.plot(S_RA_BG[:,test_k]+S_RA_HVC[:,test_k]-S_RA_exc_inh[:,test_k], label='BG+HVC-inh',lw=0.5, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA[:,test_k], label='sig(input)',lw=0, marker=',', alpha=.5)\n",
    "        # plt.plot(S_RA_HVC[:,test_k], label='HVC',lw=0.5, marker=',')\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_RA_inputs.png')\n",
    "        # plt.close()\n",
    "        \n",
    "\n",
    "        # Plot sum of dW1\n",
    "\n",
    "        # fig = plt.figure()\n",
    "        # plt.plot(S_p, lw=0, marker='.', label='pf')\n",
    "        # plt.plot(S_dW1_sum.flatten(), lw=0.2, marker=',', label='daily sum')\n",
    "        # plt.plot(R.flatten(), lw=0.2, marker=',', alpha=.1, label='R')\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_dW1_sum.png')\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "        # # Plot RA_exc vs BG\n",
    "        # fig = plt.figure()\n",
    "        # plt.hist(S_RA.ravel()[::100], label='RA', alpha=.5);\n",
    "        # plt.hist(S_BG.ravel()[::100], label='BG', alpha=.5);\n",
    "\n",
    "        # plt.legend()\n",
    "\n",
    "        # plt.savefig('Results/' + resname + '_S_RA.png')\n",
    "        # plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # Display overall results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.rc(\"ytick\", labelsize=\"small\")\n",
    "        plt.rc(\"xtick\", labelsize=\"small\")\n",
    "\n",
    "        n_subplots = 6\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 1)\n",
    "        ax.set_title(\"Testing RL with normalised sigmoid\")\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # Plot all trial rewards\n",
    "        T = np.arange(len(R.T.flatten()))\n",
    "        ax.plot(T[::10], R.T.flatten()[::10], marker=\"o\", markersize=1.5, linewidth=0, alpha=.25,\n",
    "                color=\"none\", markeredgecolor=\"none\", markerfacecolor=\"black\")\n",
    "        ax.set_xlabel(\"Trial #\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 2)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, W_RL[:,test_k:5000:1000], color='black', alpha=.5, linewidth=0.5)\n",
    "        ax.set_ylabel(\"HVC - BG weights\")\n",
    "        ax.set_ylim(Wmin_RL-0.1, Wmax_RL+0.1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 3)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_BG[:,::200], color='black', alpha=.5, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.set_ylabel(\"BG output\")\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 4)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, W_HL[:,test_k:5000:1000], color='black', alpha=.5, linewidth=0.5)\n",
    "        ax.set_ylabel(\"HVC - RA weights\")\n",
    "        ax.set_ylim(Wmin_HL-0.1, Wmax_HL+0.1) \n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 5)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_MO, color='black', alpha=.5, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.plot(T, S_T, color='red', marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.set_xlabel(\"Trials #\")\n",
    "        ax.set_ylabel(\"Output\")\n",
    "        ax.set_ylim(-1, 1)\n",
    "\n",
    "        ax = plt.subplot(n_subplots, 1, 6)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        T = np.arange(n_total_trials)\n",
    "        ax.plot(T, S_RA[:,test_k], color='black', alpha=.1, marker=',', linewidth=0, markersize=0.5)\n",
    "        # ax.plot(T, S_RA_inh, color='red', alpha=.1, marker=',', linewidth=0, markersize=0.5)\n",
    "        ax.axhline(y=0, color='red', alpha=.5, marker=',', linewidth=0.5)\n",
    "        ax.axhline(y=0.5, color='black', alpha=.5, marker=',', linewidth=0.5)\n",
    "        ax.set_ylabel(\"RA activity\")\n",
    "        ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Results/' + resname + '_overall.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "        # Plot trajectory on 2d plane\n",
    "        \"\"\" Plots the trajectory of the model output and cortical pathway over the simulation. \"\"\"\n",
    "\n",
    "        for n_syll in np.arange(n_syllables):\n",
    "            syll = syllables[n_syll]\n",
    "            contour = syllable_contours[syll]\n",
    "            target = syllable_targets[syll]\n",
    "\n",
    "\n",
    "            figure = plt.figure(figsize=(8,8))\n",
    "            ax = plt.subplot(frameon=True)\n",
    "\n",
    "            sk = n_syllables\n",
    "\n",
    "            # Plot reward contour as circles around hills    \n",
    "            contour_plot = ax.contourf(contour, 30, extent=[-1,1,-1,1], cmap=\"gray_r\", alpha=.25)\n",
    "            contour_plot = ax.contour(contour, 10, extent=[-1,1,-1,1], colors=\"black\",  alpha=.5, linewidths=0.5)\n",
    "\n",
    "            # Plot reward contour as a color plot\n",
    "            im = ax.imshow(contour, vmin=0, vmax=np.max(contour), cmap='Purples', extent=[-1,1,-1,1], origin='lower')\n",
    "\n",
    "            # Plot trajectory\n",
    "            list_colors = cm.plasma(np.linspace(0, 1, 5))\n",
    "            for day_index in np.arange(n_days+n_lesioned_days):\n",
    "                ax.plot(S_MO[n_syll+day_index*n_daily_motifs:n_syll+(day_index+1)*n_daily_motifs:sk,0],S_MO[n_syll+day_index*n_daily_motifs:n_syll+(day_index+1)*n_daily_motifs:sk,1], color=list_colors[day_index%5], lw=0, marker='.', alpha=0.1, markersize=2)\n",
    "            \n",
    "            # # Annotations\n",
    "            ax.scatter(S_MO[n_syll,0],S_MO[n_syll,1], color=\"sienna\", marker=\"o\",\n",
    "                    linewidths=3, facecolors=\"sienna\", zorder=10, label='Initial point')\n",
    "            ax.scatter(S_MO[-n_syllables+n_syll,0],S_MO[-n_syllables+n_syll,1], s=100, color=\"orange\", marker=\"x\",\n",
    "                    linewidths=3, facecolors=\"orange\", zorder=10, label='Final point')\n",
    "\n",
    "            # Display colorbar\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.5)\n",
    "            cbar = figure.colorbar(im, cax=cax, ticks=[0, .5, 1])\n",
    "            cbar.set_label('Performance metric (R)', rotation=270, fontsize=25, labelpad=25)\n",
    "            cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "            ax.set_xlabel(r'$P_{\\beta}$', fontsize=30)\n",
    "            ax.set_ylabel(r'$P_{\\alpha}$', fontsize=30)\n",
    "            ax.set_xticks(np.linspace(-1, 1, 3))\n",
    "            ax.set_yticks(np.linspace(-1, 1, 3))\n",
    "            ax.tick_params(labelsize=25)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.savefig('Results/' + resname + '_trajectory.png')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    # Returns terminal performance (avg performance over last training day, and avg performance on lesion day\n",
    "    return(np.mean(R[0,60000-1000:60000-1]), np.mean(R[0,-1000:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running multiple simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491263_annealing\n",
      "[0.04685407 0.31543704]\n",
      "491263\n",
      "[0.04685407 0.31543704]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4878661078721011, 0.4824063796875348)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Sample simulation of learning with and without annealing for a given rseed (with descriptive plots)\n",
    "\n",
    "build_and_simulate(491263, True, True)\n",
    "build_and_simulate(491263, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall seed:  0\n"
     ]
    }
   ],
   "source": [
    "#### Multiple simulation of learning with and without annealing (with or without descriptive plots)\n",
    "\n",
    "# Master seed to control the starting seeds generated for each simulation in the test batch\n",
    "overall_seed = 0\n",
    "np.random.seed(overall_seed)\n",
    "print('Overall seed: ', overall_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating starting seeds for each simulation in the test batch\n",
    "\n",
    "n_simulations = 10\n",
    "seeds = np.random.randint(0, 1e7, n_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording terminal performance of each simulation\n",
    "\n",
    "performance_annealing = np.zeros(n_simulations)\n",
    "performance_no_annealing = np.zeros(n_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8325804_annealing\n",
      "[ 0.13866744 -0.03879846]\n",
      "8325804\n",
      "[ 0.13866744 -0.03879846]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:32<04:51, 32.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484405_annealing\n",
      "[ 0.49806912 -0.31132766]\n",
      "1484405\n",
      "[ 0.49806912 -0.31132766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:03<04:11, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2215104_annealing\n",
      "[-0.16773095  0.33087046]\n",
      "2215104\n",
      "[-0.16773095  0.33087046]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:28<03:21, 28.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5157699_annealing\n",
      "[-0.46821022  0.31365896]\n",
      "5157699\n",
      "[-0.46821022  0.31365896]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:58<02:54, 29.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222403_annealing\n",
      "[ 0.29002286 -0.08836048]\n",
      "8222403\n",
      "[ 0.29002286 -0.08836048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:25<02:21, 28.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7644169_annealing\n",
      "[-0.59377638 -0.05881897]\n",
      "7644169\n",
      "[-0.59377638 -0.05881897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:02<02:04, 31.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5853461_annealing\n",
      "[-0.47912372  0.36309777]\n",
      "5853461\n",
      "[-0.47912372  0.36309777]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:40<01:40, 33.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6739698_annealing\n",
      "[0.23420799 0.49531111]\n",
      "6739698\n",
      "[0.23420799 0.49531111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:20<01:11, 35.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374564_annealing\n",
      "[-0.00112911  0.52627081]\n",
      "374564\n",
      "[-0.00112911  0.52627081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:53<00:34, 34.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2832983_annealing\n",
      "[-0.53099852  0.20654741]\n",
      "2832983\n",
      "[-0.53099852  0.20654741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:28<00:00, 32.83s/it]\n"
     ]
    }
   ],
   "source": [
    "### Testing robustness across different initial model configurations and sensorimotor landscapes\n",
    "\n",
    "for i in tqdm(np.arange(n_simulations)):\n",
    "    rseed = seeds[i]\n",
    "\n",
    "    ## Testing a given setting in both annealing and no annealing cases\n",
    "    # parameter ->(seed, annealing  - yes/no, plots - yes/no)\n",
    "    perf_score1 = build_and_simulate(rseed, True, True)                    \n",
    "    perf_score2 = build_and_simulate(rseed, False, True)\n",
    "\n",
    "    ## Terminal performance (mean performance on the last day)\n",
    "    # perf_score1[0] = avg performance on the last day before lesion of BG contribution\n",
    "    # perf_score1[1] = avg performance on the day post lesion of BG contribution\n",
    "    performance_annealing[i] = perf_score1[1]\n",
    "    performance_no_annealing[i] = perf_score2[1]\n",
    "\n",
    "np.save('Results/performance_annealing.npy', performance_annealing)\n",
    "np.save('Results/performance_no_annealing.npy', performance_no_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proportion of simulations that find the global optimum\n",
    "\n",
    "# 0.6 = maximum yield across all local optima\n",
    "# If a simulation has a terminal performance > 0.6 => the simulation must have landed on the global optimum region\n",
    "\n",
    "overall_perf_annealing = np.count_nonzero(performance_annealing>.6)/n_simulations * 100\n",
    "overall_perf_no_annealing = np.count_nonzero(performance_no_annealing>.6)/n_simulations * 100\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of terminal performances with and without annealing\n",
    "\n",
    "plt.plot(performance_annealing, linewidth=0, marker='.', color='g', label='Annealing')\n",
    "plt.plot(performance_no_annealing, linewidth=0, marker='.', color='k', label='No annealing')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.title('Score: ' + str(overall_perf_annealing) + '%,' + str(overall_perf_no_annealing) + '%')\n",
    "\n",
    "plt.savefig('Results/Overall.png')\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
